{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0isZVdZ0sf4U"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U wandb\n",
        "!pip install -q natsort tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import axes_grid1\n",
        "# import tensorflow_models as tfm \n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers \n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from google.cloud import storage\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, CSVLogger\n",
        "import tensorflow_datasets as tfds  \n",
        "\n",
        "from albumentations import (\n",
        "    Compose, RandomBrightness, JpegCompression, HueSaturationValue, RandomContrast, HorizontalFlip,\n",
        "    Rotate, Normalize\n",
        ")\n",
        "\n",
        "import tensorflow_addons as tfa"
      ],
      "metadata": {
        "id": "bAcOIDfPs90x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MIXED_PRECISION = True\n",
        "XLA_ACCELERATE = True\n",
        "\n",
        "try:  # detect TPUs\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()  \n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    DEVICE = 'TPU'\n",
        "except ValueError:  # detect GPUs\n",
        "    strategy = tf.distribute.get_strategy() \n",
        "    DEVICE = 'GPU'\n",
        "    \n",
        "if DEVICE == \"GPU\":\n",
        "    physical_devices = tf.config.list_physical_devices('GPU')\n",
        "    print(\"Num GPUs Available: \", len(physical_devices))\n",
        "    try: \n",
        "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "        assert tf.config.experimental.get_memory_growth(physical_devices[0])\n",
        "    except: # Invalid device or cannot modify virtual devices once initialized.\n",
        "        pass \n",
        "    \n",
        "if MIXED_PRECISION:\n",
        "    dtype = 'mixed_bfloat16' if DEVICE == \"TPU\" else 'mixed_float16'\n",
        "    tf.keras.mixed_precision.set_global_policy(dtype)\n",
        "    dtype_model = tf.bfloat16\n",
        "    print('Mixed precision enabled')\n",
        "else:\n",
        "    dtype_model = tf.float32\n",
        "\n",
        "\n",
        "if XLA_ACCELERATE:\n",
        "    tf.config.optimizer.set_jit(True)\n",
        "    print('Accelerated Linear Algebra enabled')\n",
        "    \n",
        "AUTO  = tf.data.AUTOTUNE\n",
        "REPLICAS = strategy.num_replicas_in_sync\n",
        "\n",
        "print('REPLICAS           : ', REPLICAS)\n",
        "print('TensorFlow Version : ', tf.__version__)\n",
        "print('Eager Mode Status  : ', tf.executing_eagerly())\n",
        "print('TF Cuda Built Test : ', tf.test.is_built_with_cuda)\n",
        "print(\n",
        "    'TF Device Detected : ', \n",
        "    'Running on TPU' if DEVICE == \"TPU\" else tf.test.gpu_device_name()\n",
        ")\n",
        "\n",
        "try:\n",
        "    print('TF System Cuda V.  : ', tf.sysconfig.get_build_info()[\"cuda_version\"])\n",
        "    print('TF System CudNN V. : ', tf.sysconfig.get_build_info()[\"cudnn_version\"])\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "UVv7-JNZtAag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR_PATH = 'gs://kds-0fe1708952e30744d498ab5049717990e1a2816a0f9ba56acf700c28'"
      ],
      "metadata": {
        "id": "gCemtPKPtB6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_shard_suffix = 'data_*-9.tfrec'\n",
        "train_set_path = sorted(tf.io.gfile.glob(DATA_DIR_PATH + f'/{train_shard_suffix}'))\n",
        "\n",
        "batch_size = 128 * REPLICAS\n",
        "batch_size_fer = 32 * REPLICAS\n",
        "image_shape = (256, 256)\n",
        "\n",
        "train_set_len = 202599 # for part 0 and for part 1: 655167\n",
        "train_step_epoch = -(-train_set_len // batch_size)\n",
        "\n",
        "val_set_len = 28662\n",
        "val_step_epoch = -(-val_set_len // batch_size)"
      ],
      "metadata": {
        "id": "KRt6fkXitDeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GCS_DS_PATH_fer = \"gs://kds-e0b005301382bc95f485d3d97efd910f02aa1f607d7e1f74cf878f2f\"\n",
        "train_shard_suffix = 'train_*-3.tfrec'\n",
        "train_set_path_fer = sorted(tf.io.gfile.glob(GCS_DS_PATH_fer + f'/train/{train_shard_suffix}'))"
      ],
      "metadata": {
        "id": "hBdwOkR-tEwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Augmentation(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Augmentation, self).__init__()\n",
        "    @tf.function\n",
        "    def random_execute(self, prob: float) -> bool:\n",
        "        return tf.random.uniform([], minval=0, maxval=1) < prob\n",
        "\n",
        "class RandomToGrayscale(Augmentation):\n",
        "    @tf.function\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        if self.random_execute(0.2):\n",
        "            x = tf.image.rgb_to_grayscale(x)\n",
        "            x = tf.tile(x, [1, 1, 3])\n",
        "        return x\n",
        "\n",
        "class RandomColorJitter(Augmentation):\n",
        "    @tf.function\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        if self.random_execute(0.8):\n",
        "            x = tf.image.random_brightness(x, 0.8)\n",
        "            x = tf.image.random_contrast(x, 0.2, 0.8)\n",
        "            x = tf.image.random_saturation(x, 0.4, 1.6)\n",
        "            x = tf.image.random_hue(x, 0.2)\n",
        "        return x\n",
        "\n",
        "class RandomFlip(Augmentation):\n",
        "    @tf.function\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        if self.random_execute(0.8):\n",
        "            x = tf.image.random_flip_left_right(x)\n",
        "        return x\n",
        "\n",
        "class RandomResizedCrop(Augmentation):\n",
        "    def __init__(self, image_size):\n",
        "        super(Augmentation, self).__init__()\n",
        "        self.image_size = image_size\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        rand_size = tf.random.uniform(\n",
        "            shape=[],\n",
        "            minval=int(0.75 * self.image_size),\n",
        "            maxval=1 * self.image_size,\n",
        "            dtype=tf.int32,\n",
        "        )\n",
        "        crop = tf.image.random_crop(x, (rand_size, rand_size, 3))\n",
        "        crop_resize = tf.image.resize(crop, (self.image_size, self.image_size))\n",
        "        return crop_resize\n",
        "\n",
        "class RandomSolarize(Augmentation):\n",
        "    @tf.function\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        if self.random_execute(0.2):\n",
        "            # flips abnormally low pixels to abnormally high pixels\n",
        "            x = tf.where(x < 10, x, 255 - x)\n",
        "        return x\n",
        "\n",
        "class RandomBlur(Augmentation):\n",
        "    @tf.function\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        if self.random_execute(0.2):\n",
        "            s = np.random.random()\n",
        "            return tfa.image.gaussian_filter2d(image=x, sigma=s)\n",
        "        return x\n",
        "\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "import math\n",
        "IMAGE_SIZE = [256,256]\n",
        "\n",
        "def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
        "    # returns 3x3 transformmatrix which transforms indicies\n",
        "        \n",
        "    # CONVERT DEGREES TO RADIANS\n",
        "    rotation = math.pi * rotation / 180.\n",
        "    shear = math.pi * shear / 180.\n",
        "    \n",
        "    # ROTATION MATRIX\n",
        "    c1 = tf.math.cos(rotation)\n",
        "    s1 = tf.math.sin(rotation)\n",
        "    one = tf.constant([1],dtype='float32')\n",
        "    zero = tf.constant([0],dtype='float32')\n",
        "    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n",
        "        \n",
        "    # SHEAR MATRIX\n",
        "    c2 = tf.math.cos(shear)\n",
        "    s2 = tf.math.sin(shear)\n",
        "    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n",
        "\n",
        "    # ZOOM MATRIX\n",
        "    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n",
        "    \n",
        "    # SHIFT MATRIX\n",
        "    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n",
        "    \n",
        "    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\n",
        "\n",
        "def transform(image):\n",
        "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
        "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
        "    DIM = IMAGE_SIZE[0]\n",
        "    XDIM = DIM%2 #fix for size 331\n",
        "    \n",
        "    rot = 15. * tf.random.normal([1],dtype='float32')\n",
        "    shr = 5. * tf.random.normal([1],dtype='float32') \n",
        "    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n",
        "    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n",
        "    h_shift = 16. * tf.random.normal([1],dtype='float32') \n",
        "    w_shift = 16. * tf.random.normal([1],dtype='float32') \n",
        "\n",
        "    # GET TRANSFORMATION MATRIX\n",
        "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n",
        "\n",
        "    # LIST DESTINATION PIXEL INDICES\n",
        "    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n",
        "    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n",
        "    z = tf.ones([DIM*DIM],dtype='int32')\n",
        "    idx = tf.stack( [x,y,z] )\n",
        "    \n",
        "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
        "    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n",
        "    idx2 = K.cast(idx2,dtype='int32')\n",
        "    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n",
        "    \n",
        "    # FIND ORIGIN PIXEL VALUES           \n",
        "    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n",
        "    d = tf.gather_nd(image,tf.transpose(idx3))\n",
        "        \n",
        "    return tf.reshape(d,[DIM,DIM,3])\n",
        "\n",
        "class RandomRotage(Augmentation):\n",
        "    @tf.function\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        if self.random_execute(0.7):\n",
        "            # flips abnormally low pixels to abnormally high pixels\n",
        "            # x = tf.where(x < 10, x, 255 - x)\n",
        "            x = transform(x)\n",
        "        return x\n",
        "\n",
        "class RandomAugmentor(keras.Model):\n",
        "    def __init__(self, image_size: int):\n",
        "        super(RandomAugmentor, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.random_resized_crop = RandomResizedCrop(image_size)\n",
        "        self.random_flip = RandomFlip()\n",
        "        self.random_color_jitter = RandomColorJitter()\n",
        "        self.random_blur = RandomBlur()\n",
        "        self.random_to_grayscale = RandomToGrayscale()\n",
        "        self.random_solarize = RandomSolarize()\n",
        "        self.random_rotage = RandomRotage()\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "#         x = self.random_resized_crop(x)\n",
        "        x = self.random_rotage(x)\n",
        "        x = self.random_flip(x)\n",
        "        x = self.random_color_jitter(x)\n",
        "        x = self.random_blur(x)\n",
        "        x = self.random_to_grayscale(x)\n",
        "        # x = self.random_solarize(x)\n",
        "#         x = tf.clip_by_value(x, 0, 1)\n",
        "        return x\n",
        "\n",
        "bt_augmentor = RandomAugmentor(image_shape[0])"
      ],
      "metadata": {
        "id": "d5ybpDM9tHO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def deserialization_fn(serialized_example):\n",
        "    features = tf.io.parse_single_example(\n",
        "        serialized_example,\n",
        "        features={\n",
        "            'image': tf.io.FixedLenFeature([], tf.string),\n",
        "        })\n",
        "    \n",
        "    image = tf.image.decode_jpeg(features['image'], channels=3)\n",
        "    image = tf.image.resize(image, image_shape)\n",
        "    image_1 = bt_augmentor(image)\n",
        "    image_2 = bt_augmentor(image)\n",
        "\n",
        "    image_1 = tf.keras.applications.mobilenet_v2.preprocess_input(image_1)\n",
        "    image_2 = tf.keras.applications.mobilenet_v2.preprocess_input(image_2)\n",
        "    return image_1, image_2"
      ],
      "metadata": {
        "id": "buMM0x4YtITm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tfrecords_loader(files_path, shuffle=False):\n",
        "    datasets = tf.data.Dataset.from_tensor_slices(files_path)\n",
        "    datasets = datasets.shuffle(len(files_path)) if shuffle else datasets\n",
        "    datasets = datasets.flat_map(tf.data.TFRecordDataset)\n",
        "    datasets = datasets.map(deserialization_fn, num_parallel_calls=AUTO)\n",
        "    return datasets\n",
        "\n",
        "train_datasets = tfrecords_loader(train_set_path)\n",
        "train_datasets,"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aFAvhmOtKW3",
        "outputId": "0d10065f-eccc-48d9-8f6b-c952ca936166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<ParallelMapDataset element_spec=(TensorSpec(shape=(256, 256, 3), dtype=tf.bfloat16, name=None), TensorSpec(shape=(256, 256, 3), dtype=tf.bfloat16, name=None))>,)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tfrecords_loader_fer(files_path, shuffle=False):\n",
        "    datasets = tf.data.Dataset.from_tensor_slices(files_path)\n",
        "    datasets = datasets.shuffle(len(files_path)) if shuffle else datasets\n",
        "    datasets = datasets.flat_map(tf.data.TFRecordDataset)\n",
        "    \n",
        "    def deserialization_fn_fer(serialized_example):\n",
        "        features = tf.io.parse_single_example(\n",
        "            serialized_example,\n",
        "            features={\n",
        "                'image': tf.io.FixedLenFeature([], tf.string),\n",
        "                'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "            })\n",
        "\n",
        "        image = tf.image.decode_jpeg(features['image'], channels=3)\n",
        "        image = tf.image.resize(image, image_shape)\n",
        "        \n",
        "        image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
        "        label = tf.cast(features['label'], tf.int64)  # [0-999]\n",
        "\n",
        "        return image, label\n",
        "    datasets = datasets.map(deserialization_fn_fer, num_parallel_calls=AUTO)\n",
        "    return datasets"
      ],
      "metadata": {
        "id": "Vbv2EEIrtL8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datasets_fer = tfrecords_loader_fer(train_set_path_fer)\n",
        "train_datasets_fer"
      ],
      "metadata": {
        "id": "BBG1fEldtM-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset_encode = train_datasets.repeat().batch(batch_size).prefetch(AUTO)\n",
        "# train_dataset_fer_encode = train_datasets_fer.repeat().batch(batch_size).prefetch(AUTO)\n",
        "def get_dataset(dataset, batch_size):\n",
        "    return dataset.repeat().batch(batch_size).prefetch(AUTO)"
      ],
      "metadata": {
        "id": "ji1k561vtOIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cp_callback = ModelCheckpoint(filepath='/tmp/checkpoints/model.{epoch:02d}-{loss:.2f}.hdf5',\n",
        "                             monitor='loss',\n",
        "                             save_freq='epoch',\n",
        "                             verbose=1,\n",
        "                             period=20,\n",
        "                             save_best_only=True,\n",
        "                             save_weights_only=True)\n"
      ],
      "metadata": {
        "id": "bMDqN-3htPpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_lr = 0.001\n",
        "min_lr = 0.0005\n",
        "max_lr = 0.005\n",
        "rampup_epochs = 10\n",
        "sustain_epochs = 0\n",
        "exp_decay = .8\n",
        "EPOCHS = 200\n",
        "\n",
        "def lrfn(epoch):\n",
        "  if epoch < rampup_epochs:\n",
        "    return (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n",
        "  elif epoch < rampup_epochs + sustain_epochs:\n",
        "    return max_lr\n",
        "  else:\n",
        "    return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n",
        "\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True)\n",
        "\n",
        "rang = np.arange(EPOCHS)\n",
        "y = [lrfn(x) for x in rang]\n",
        "plt.plot(rang, y)\n",
        "print('Learning rate per epoch:')"
      ],
      "metadata": {
        "id": "00B6m3zItRe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /tmp/tensorboard\n",
        "!rm -rf /tmp/checkpoints\n",
        "\n",
        "!mkdir /tmp/tensorboard\n",
        "!mkdir /tmp/checkpoints"
      ],
      "metadata": {
        "id": "SaUrsU21tSdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from natsort import natsorted\n",
        "\n",
        "class WandbCustom:\n",
        "    def __init__(self, path_ckpt, fred=1):\n",
        "        self.path_ckpt = path_ckpt\n",
        "        # self.old_name = \"\"\n",
        "        self.counter = 0\n",
        "        self.fred = fred\n",
        "    def upload_file_to_gcs(self, src_path: str):\n",
        "        \n",
        "        artifact = wandb.Artifact('model', type='model')\n",
        "        artifact.add_dir(src_path)\n",
        "        wandb.log_artifact(artifact)\n",
        "    \n",
        "    def __call__(self):\n",
        "        \n",
        "        print(self.counter % self.fred)\n",
        "        if self.counter % self.fred == 1:\n",
        "            \n",
        "            self.upload_file_to_gcs(self.path_ckpt)\n",
        "            print(f\"Uploaded {self.path_ckpt}\\n\")\n",
        "        self.counter += 1\n",
        "        \n",
        "        "
      ],
      "metadata": {
        "id": "R-k8hPXetT6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbMetricsLogger\n",
        "from wandb.keras import WandbCallback\n",
        "run = wandb.init(project=\"...\", name=\"...\")"
      ],
      "metadata": {
        "id": "EWrIy00CtVcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def off_diagonal(x):\n",
        "    n = tf.shape(x)[0]\n",
        "    flattened = tf.reshape(x, [-1])[:-1]\n",
        "    off_diagonals = tf.reshape(flattened, (n-1, n+1))[:, 1:]\n",
        "    return tf.reshape(off_diagonals, [-1])\n",
        "\n",
        "def normalize_repr(z):\n",
        "    z_norm = (z - tf.reduce_mean(z, axis=0)) / tf.math.reduce_std(z, axis=0)\n",
        "    return z_norm\n",
        "\n",
        "def compute_loss(z_a, z_b, lambd=5e-3):\n",
        "    # Get batch size and representation dimension.\n",
        "    batch_size = tf.cast(tf.shape(z_a)[0], z_a.dtype)\n",
        "    repr_dim = tf.shape(z_a)[1]\n",
        "\n",
        "    # Normalize the representations along the batch dimension.\n",
        "    z_a_norm = normalize_repr(z_a)\n",
        "    z_b_norm = normalize_repr(z_b)\n",
        "\n",
        "    # Cross-correlation matrix.\n",
        "    c = tf.matmul(z_a_norm, z_b_norm, transpose_a=True) / batch_size\n",
        "\n",
        "    # Loss.\n",
        "    on_diag = tf.linalg.diag_part(c) + (-1)\n",
        "    on_diag = tf.reduce_sum(tf.pow(on_diag, 2))\n",
        "    off_diag = off_diagonal(c)\n",
        "    off_diag = tf.reduce_sum(tf.pow(off_diag, 2))\n",
        "    loss = on_diag + (lambd * off_diag)\n",
        "    return loss    "
      ],
      "metadata": {
        "id": "FlNQZCDFtXRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch_size = 4  # 2x2, for the Transformer blocks.\n",
        "image_size = 256\n",
        "expansion_factor = 4  # expansion factor for the MobileNetV2 blocks.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def conv_block(x, filters=16, kernel_size=3, strides=2):\n",
        "    x = layers.Conv2D(\n",
        "        filters, kernel_size, strides=strides, padding=\"same\"\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization(momentum=0.1)(x)\n",
        "    x = tf.nn.swish(x)\n",
        "    return x\n",
        "\n",
        "def inverted_residual_block(x, expanded_channels, output_channels, strides=1):\n",
        "    m = layers.Conv2D(expanded_channels, 1, padding=\"same\", use_bias=False)(x)\n",
        "    m = layers.BatchNormalization(momentum=0.1)(m)\n",
        "    m = tf.nn.swish(m)\n",
        "\n",
        "    if strides == 2:\n",
        "        m = layers.ZeroPadding2D(padding=correct_pad(m, 3))(m)\n",
        "    m = layers.DepthwiseConv2D(\n",
        "        3, strides=strides, padding=\"same\" if strides == 1 else \"valid\", use_bias=False\n",
        "    )(m)\n",
        "    m = layers.BatchNormalization(momentum=0.1)(m)\n",
        "    m = tf.nn.swish(m)\n",
        "\n",
        "    m = layers.Conv2D(output_channels, 1, padding=\"same\", use_bias=False)(m)\n",
        "    m = layers.BatchNormalization(momentum=0.1)(m)\n",
        "\n",
        "    if tf.math.equal(x.shape[-1], output_channels) and strides == 1:\n",
        "        return layers.Add()([m, x])\n",
        "    return m\n",
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.swish)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def transformer_block(x, transformer_layers, projection_dim, num_heads=2):\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, x])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=[x.shape[-1] * 2, x.shape[-1]], dropout_rate=0.1,)\n",
        "        # Skip connection 2.\n",
        "        x = layers.Add()([x3, x2])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def mobilevit_block(x, num_blocks, projection_dim, strides=1):\n",
        "    # Local projection with convolutions.\n",
        "    local_features = conv_block(x, filters=projection_dim, strides=strides)\n",
        "    se_features = SqueezeExcitation(projection_dim, projection_dim, 0.25)(local_features)\n",
        "    local_features = conv_block(\n",
        "        local_features, filters=projection_dim, kernel_size=1, strides=strides\n",
        "    )\n",
        "\n",
        "    # Unfold into patches and then pass through Transformers.\n",
        "    num_patches = int((local_features.shape[1] * local_features.shape[2]) / patch_size)\n",
        "    non_overlapping_patches = layers.Reshape((patch_size, num_patches, projection_dim))(\n",
        "        local_features\n",
        "    )\n",
        "    global_features = transformer_block(\n",
        "        non_overlapping_patches, num_blocks, projection_dim\n",
        "    )\n",
        "\n",
        "    # Fold into conv-like feature-maps.\n",
        "    folded_feature_map = layers.Reshape((*local_features.shape[1:-1], projection_dim))(\n",
        "        global_features\n",
        "    )\n",
        "\n",
        "    # Apply point-wise conv -> concatenate with the input features.\n",
        "    folded_feature_map = conv_block(\n",
        "        folded_feature_map, filters=x.shape[-1], kernel_size=1, strides=strides\n",
        "    )\n",
        "    local_global_features = layers.Concatenate(axis=-1)([x, folded_feature_map, se_features])\n",
        "\n",
        "    # Fuse the local and global features using a convoluion layer.\n",
        "    local_global_features = conv_block(\n",
        "        local_global_features, filters=projection_dim, strides=strides\n",
        "    )\n",
        "\n",
        "    return local_global_features\n",
        "\n",
        "class SqueezeExcitation(tf.keras.layers.Layer):\n",
        "  \"\"\"Creates a squeeze and excitation layer.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               in_filters,\n",
        "               out_filters,\n",
        "               se_ratio,\n",
        "               **kwargs):\n",
        "\n",
        "    super(SqueezeExcitation, self).__init__(**kwargs)\n",
        "\n",
        "    self._in_filters = in_filters\n",
        "    self._out_filters = out_filters\n",
        "    self._se_ratio = se_ratio\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    num_reduced_filters = max(1, int(self._in_filters * self._se_ratio))\n",
        "    self._se_reduce = tf.keras.layers.Conv2D(\n",
        "        filters=num_reduced_filters,\n",
        "        kernel_size=1,\n",
        "        strides=1,\n",
        "        padding='same',\n",
        "        use_bias=True,\n",
        "        )\n",
        "\n",
        "    self._se_expand = tf.keras.layers.Conv2D(\n",
        "        filters=self._out_filters,\n",
        "        kernel_size=1,\n",
        "        strides=1,\n",
        "        padding='same',\n",
        "        use_bias=True,\n",
        "        )\n",
        "\n",
        "    super(SqueezeExcitation, self).build(input_shape)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = {\n",
        "        'in_filters': self._in_filters,\n",
        "        'out_filters': self._out_filters,\n",
        "        'se_ratio': self._se_ratio,\n",
        "    }\n",
        "    base_config = super(SqueezeExcitation, self).get_config()\n",
        "    return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = tf.reduce_mean(inputs, [1, 2], keepdims=True)\n",
        "    x = tf.nn.swish(self._se_reduce(x))\n",
        "    x = tf.nn.sigmoid(self._se_expand(x))\n",
        "    return x * inputs\n",
        "\n",
        "def correct_pad(inputs, kernel_size):\n",
        "    \"\"\"Returns a tuple for zero-padding for 2D convolution with downsampling.\n",
        "    Args:\n",
        "      inputs: Input tensor.\n",
        "      kernel_size: An integer or tuple/list of 2 integers.\n",
        "    Returns:\n",
        "      A tuple.\n",
        "    \"\"\"\n",
        "    img_dim = 2 if tf.keras.backend.image_data_format() == \"channels_first\" else 1\n",
        "    input_size = tf.keras.backend.int_shape(inputs)[img_dim : (img_dim + 2)]\n",
        "    if isinstance(kernel_size, int):\n",
        "        kernel_size = (kernel_size, kernel_size)\n",
        "    if input_size[0] is None:\n",
        "        adjust = (1, 1)\n",
        "    else:\n",
        "        adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)\n",
        "    correct = (kernel_size[0] // 2, kernel_size[1] // 2)\n",
        "    return (\n",
        "        (correct[0] - adjust[0], correct[0]),\n",
        "        (correct[1] - adjust[1], correct[1]),\n",
        "    )\n",
        "    \n",
        "\n",
        "def create_mobilevit(num_classes=1000):\n",
        "    inputs = keras.Input((image_size, image_size, 3))\n",
        "    # x = layers.Rescaling(scale=1.0 / 255)(inputs)\n",
        "\n",
        "    # Initial conv-stem -> MV2 block.\n",
        "    x = conv_block(inputs, filters=16)\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=16 * expansion_factor, output_channels=32\n",
        "    )\n",
        "\n",
        "    # Downsampling with MV2 block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=32 * expansion_factor, output_channels=48, strides=2\n",
        "    )\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=48 * expansion_factor, output_channels=48\n",
        "    )\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=48 * expansion_factor, output_channels=48\n",
        "    )\n",
        "\n",
        "    # First MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=48 * expansion_factor, output_channels=64, strides=2\n",
        "    )\n",
        "    # x = SqueezeExcitation(48, 48, 0.25)(x)\n",
        "    x = mobilevit_block(x, num_blocks=2, projection_dim=96)\n",
        "    \n",
        "\n",
        "    # Second MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=80 * expansion_factor, output_channels=80, strides=2\n",
        "    )\n",
        "    # x = SqueezeExcitation(64, 64, 0.25)(x)\n",
        "    x = mobilevit_block(x, num_blocks=4, projection_dim=120)\n",
        "    \n",
        "\n",
        "    # Third MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=96 * expansion_factor, output_channels=96, strides=2\n",
        "    )\n",
        "    # x = SqueezeExcitation(80, 80, 0.25)(x)\n",
        "    x = mobilevit_block(x, num_blocks=3, projection_dim=144)\n",
        "    x = conv_block(x, filters=384, kernel_size=1, strides=1)\n",
        "\n",
        "    # Classification head.\n",
        "    x = layers.GlobalAvgPool2D()(x)\n",
        "    outputs = layers.Dense(num_classes,activation='softmax', dtype='float32')(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)\n"
      ],
      "metadata": {
        "id": "9XK3kmT_tbIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "def get_encoder():\n",
        "    mobile = create_mobilevit()\n",
        "\n",
        "\n",
        "    # dir_work = \"gs://cuong_tpu/logs/mobilevit_SE/\"\n",
        "    # ckpt = tf.train.Checkpoint(model=mobile)\n",
        "    # ckpt.restore(tf.train.latest_checkpoint(dir_work+'ckpts'))\n",
        "\n",
        "    last_layer = mobile.layers[-2].output\n",
        "\n",
        "    model = keras.Model(mobile.input, last_layer)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "lmO4CeUItYsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_projection_head(dims=[1280, 1024*2, 1024*5, 1024*5]):\n",
        "    return keras.Sequential(\n",
        "        [\n",
        "            keras.Input(shape=(dims[0],)),\n",
        "            keras.layers.Dense(dims[1]),\n",
        "            keras.layers.BatchNormalization(),\n",
        "            keras.layers.ReLU(),\n",
        "            keras.layers.Dense(dims[2]),\n",
        "            keras.layers.BatchNormalization(),\n",
        "            keras.layers.ReLU(),\n",
        "            keras.layers.Dense(dims[3]),\n",
        "        ],\n",
        "        name=\"projection_head\",\n",
        "    )"
      ],
      "metadata": {
        "id": "DK5l-TKKtZxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_linear_probe(z_dim):\n",
        "    return keras.Sequential(\n",
        "        [\n",
        "            keras.layers.Input(shape=(z_dim,)), \n",
        "            keras.layers.Dense(7)],\n",
        "        name=\"linear_probe\"\n",
        "    )"
      ],
      "metadata": {
        "id": "ThqcJB4WtcHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z_dim = 384\n",
        "with strategy.scope():\n",
        "    encoder = get_encoder()\n",
        "    projection_head = get_projection_head([z_dim, 1024*2, 1024*2, 1024*5])\n",
        "    linear_probe = get_linear_probe(z_dim)\n",
        "\n",
        "    main_optimizer=tfa.optimizers.LAMB()\n",
        "    probe_optimizer=keras.optimizers.Adam()\n",
        "\n",
        "    main_loss_tracker = keras.metrics.Mean(name=\"c_loss\")\n",
        "    probe_loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n",
        "    probe_accuracy = keras.metrics.SparseCategoricalAccuracy(name=\"p_acc\")\n",
        "\n",
        "per_replica_batch_size = batch_size // strategy.num_replicas_in_sync\n",
        "per_replica_batch_size_fer = batch_size_fer // strategy.num_replicas_in_sync\n",
        "\n",
        "train_dataset = strategy.experimental_distribute_datasets_from_function(\n",
        "    lambda _: get_dataset(train_datasets, per_replica_batch_size))\n",
        "\n",
        "valid_dataset = strategy.experimental_distribute_datasets_from_function(\n",
        "    lambda _: get_dataset(train_datasets_fer, per_replica_batch_size_fer))\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(iterator):\n",
        "    def step_fn(batch):\n",
        "        y_a, y_b = batch\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_a_1, z_b_1 = encoder(y_a, training=True), encoder(y_b, training=True)\n",
        "            z_a_2, z_b_2 = projection_head(z_a_1, training=True), projection_head(z_b_1, training=True)\n",
        "            main_loss = compute_loss(z_a_2, z_b_2)\n",
        "            # main_loss = tf.nn.compute_average_loss(main_loss, global_batch_size=batch_size)\n",
        "\n",
        "        gradients = tape.gradient(main_loss, \n",
        "                encoder.trainable_weights + projection_head.trainable_weights)\n",
        "        main_optimizer.apply_gradients(\n",
        "            zip(\n",
        "                gradients,\n",
        "                encoder.trainable_weights + projection_head.trainable_weights,\n",
        "            )\n",
        "        )\n",
        "        main_loss_tracker.update_state(main_loss)\n",
        "\n",
        "    strategy.run(step_fn, args=(next(iterator),))\n",
        "\n",
        "@tf.function\n",
        "def valid_step(iterator):\n",
        "    def step_fn(batch):\n",
        "        imgs, labels = batch\n",
        "        with tf.GradientTape() as tape:\n",
        "            features = encoder(imgs, training=False)\n",
        "            class_logits = linear_probe(features, training=True)\n",
        "            loss = probe_loss(labels, class_logits)\n",
        "        gradients = tape.gradient(loss, linear_probe.trainable_weights)\n",
        "        probe_optimizer.apply_gradients(\n",
        "            zip(gradients, linear_probe.trainable_weights)\n",
        "        )\n",
        "        probe_accuracy.update_state(labels, class_logits)\n",
        "\n",
        "    strategy.run(step_fn, args=(next(iterator),))\n",
        "\n",
        "train_iterator = iter(train_dataset)\n",
        "valid_iterator = iter(valid_dataset)\n",
        "import time \n",
        "time1 = time.time()\n",
        "local_device_option = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n",
        "path_checkpoint = \"/content/log/mobilevit_SE_no\"\n",
        "ckpt = tf.train.Checkpoint(step=tf.Variable(1), encoder=encoder)\n",
        "manager = tf.train.CheckpointManager(ckpt, path_checkpoint, max_to_keep=1)\n",
        "wandb_custom = WandbCustom(path_checkpoint, 5)"
      ],
      "metadata": {
        "id": "9k4bn8SKtemG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCH = 500\n",
        "for epoch in range(EPOCH):\n",
        "    ckpt.restore(manager.latest_checkpoint, options=local_device_option)\n",
        "    if manager.latest_checkpoint:\n",
        "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
        "    else:\n",
        "        print(\"Initializing from scratch.\")\n",
        "\n",
        "    print('Epoch: {}/{}'.format(epoch, EPOCH))\n",
        "\n",
        "    for step in range(train_step_epoch):\n",
        "        train_step(train_iterator)\n",
        "    for step in range(val_step_epoch):\n",
        "        valid_step(valid_iterator)\n",
        "\n",
        "    step_running = main_optimizer.iterations.numpy()\n",
        "    loss_running = round(float(main_loss_tracker.result()), 4)\n",
        "    acc_running = round(float(probe_accuracy.result()), 2)\n",
        "    time_running = time.time()-time1\n",
        "\n",
        "    wandb.log(\n",
        "        {\n",
        "            \"epoch\": epoch,\n",
        "            \"step_running\": step_running,\n",
        "            \"loss_running\": loss_running,\n",
        "            \"acc_running\": acc_running,\n",
        "            \"time_running\": time_running,   \n",
        "        }\n",
        "    )\n",
        "    main_loss_tracker.reset_states()\n",
        "    probe_accuracy.reset_states()\n",
        "    time1 = time.time()\n",
        "    manager.save(options=local_device_option)\n",
        "    wandb_custom()\n",
        "\n",
        "    \n",
        "    print('Current step: {}, training loss: {}, accuracy: {}, time: {}'.format(\n",
        "            epoch,\n",
        "            loss_running, \n",
        "            acc_running, \n",
        "            time_running))\n",
        "    "
      ],
      "metadata": {
        "id": "YQIZTTiptiCu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}